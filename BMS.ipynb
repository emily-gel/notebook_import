{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks for BMS\n",
    "\n",
    "\n",
    "##  Contents:\n",
    "\n",
    "* [Custom PRS estimation in ALS](#custom)\n",
    "  * [Create ALS cohort](#als-cohort)\n",
    "    * [Modules and functions for LabKey](#labkey)\n",
    "    * [Participants recruited for ALS](#recruited-als)\n",
    "    * [Participants with ALS who were recruited for other diseases](#recruited-other)\n",
    "    * [Make control cohort](#control)\n",
    "    * [Get gVCF locations](#gvcf)\n",
    "  * [Bring in PRS stats data](#prs-data)\n",
    "  * [Import Docker image using Singularity](#singularity)\n",
    "* [Germline variants in oncology cohorts](#germline-survival)\n",
    "  * [Build cohorts of participants with particular germline variants](#germline-cohort)\n",
    "  * [Getting data from LabKey for survival analysis](#survival-data)\n",
    "* [Somatic variants association with survival](#somatic-survival)\n",
    "\n",
    "##  Custom PRS estimation in ALS <a name=\"custom\"></a>\n",
    "\n",
    "Task:\n",
    "* We would like to bring in our own data (PRS statistics) and apply to ALS cohort in GEL. These are currently in an S3 bucket in the format:  \n",
    "  `phenotype chrom pos ref alt pvalue beta se t_stat nobs note`\n",
    "* This would help outline to us the mechanisms of how to setup a container based (or similar environment) system to interact with external data. There is an existing container in Docker which uses gVCFs as input.\n",
    "* This would also outline how to construct cohort using the right tables in labkey and extract genotypes for that cohort to estimate PRS\n",
    "\n",
    "Input:\n",
    "* PRS statistics table\n",
    "* Phenotype\n",
    "\n",
    "Output:\n",
    "* Case-control cohort of participants with/without ALS with participant IDs and locations of gVCFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ALS cohort <a name=\"als-cohort\"></a>\n",
    "\n",
    "Creating cohorts was covered in a previous training session on [Building cohorts based on phenotypes](https://research-help.genomicsengland.co.uk/display/GERE/Building+a+cohort+based+on+phenotypes%2C+May+2022). Here we will summarise the relevant information from that to build ALS cohorts.\n",
    "\n",
    "#### Modules and functions for Labkey <a name=\"labkey\"></a>\n",
    "\n",
    "This first section imports the relevant modules, defines a function `labkey_to_df` that queries labkey with the defined SQL query, and defines the version of the main programme to use - you should update then when a new data release comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import labkey\n",
    "import pandas as pd\n",
    "\n",
    "def labkey_to_df(sql_query, database, maxrows):\n",
    "    \"\"\"generate an pandas dataframe from labkey sql query\n",
    "    Args:\n",
    "        sql_query (str): an sql query as string.\n",
    "        database (str): The name of and path to the database as a string, \n",
    "            for example  \"main-programme/main-programme_v15_2022-05-26\"\n",
    "        maxrows: the maximum number of rows to return\n",
    "    \"\"\"\n",
    "    \n",
    "    server_context = labkey.utils.create_server_context(\n",
    "        domain = \"labkey-embassy.gel.zone\",\n",
    "        container_path = database,\n",
    "        context_path = \"labkey\",\n",
    "        use_ssl = True\n",
    "    )\n",
    "    \n",
    "    results =  labkey.query.execute_sql(\n",
    "        server_context,\n",
    "        schema_name = \"lists\",\n",
    "        sql = sql_query,\n",
    "        max_rows = maxrows\n",
    "    )\n",
    "    \n",
    "    return(pd.DataFrame(results['rows']))\n",
    "\n",
    "version = \"main-programme/main-programme_v16_2022-10-13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participants recruited for ALS <a name=\"recruited-als\"></a>\n",
    "\n",
    "A number of participants were recruited to the rare disease programme for ALS and can be found with a simple query to the `rare_disease_participant_diseases` table. The following code fetches the participant IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = \"Amyotrophic lateral sclerosis or motor neuron disease\"\n",
    "\n",
    "rd_sql = (f''' SELECT participant_id, specific_disease \n",
    "    FROM rare_diseases_participant_disease \n",
    "    WHERE rare_diseases_participant_disease.specific_disease = '{disease}'\n",
    "    ''')\n",
    "\n",
    "rd_query = labkey_to_df(rd_sql, version, 1000)\n",
    "rd_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particpants with ALS who were recruited for other diseases <a name=\"recruited-other\"></a>\n",
    "\n",
    "To find additional individuals who have been enrolled for diseases other than ALS, but had an ICD-10 code indicative of ALS (G12.2) recorded in their hospital data, which can contain more recent visits, we can also query the `hes` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hes_tables = [\"apc\", \"op\"]\n",
    "icd_code = \"G122\"\n",
    "\n",
    "concatenated = []\n",
    "\n",
    "for hes_table in hes_tables:\n",
    "    sqlstr = (\n",
    "        f'''\n",
    "        SELECT participant_id, diag_all\n",
    "        FROM hes_{hes_table}\n",
    "        WHERE diag_all LIKE '%{icd_code}%'\n",
    "        '''\n",
    "    )\n",
    "    query = labkey_to_df(sqlstr, version, 100000)\n",
    "    concatenated += list(query['participant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the two lists to get a full list of participants with ALS and filter to give only unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_als = set(list(rd_query['participant_id']) + concatenated)\n",
    "print (\"Count: \", len(all_als), \"\\n\", all_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make control cohort <a name=\"control\"></a>\n",
    "\n",
    "We can get a group from the participant table and check its covariates to see if it matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sql = (f'''\n",
    "    SELECT participant_id, participant_phenotypic_sex, participant_ethnic_category, year_of_birth\n",
    "    FROM participant\n",
    "    WHERE participant_id NOT IN {*all_als,}\n",
    "''')\n",
    "\n",
    "control_query = labkey_to_df(control_sql, version, 1000)\n",
    "\n",
    "control_list = list(control_query['participant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MatchIt R algorithm is available to ensure matching between case and control cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get gVCF locations <a name=\"gvcf\"></a>\n",
    "\n",
    "To get the gVCFs for PRS analysis, you can find the location of these using the `genome_file_paths_and_types` table. You can then incorporate the files into your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filetype = \"Genomic VCF\"\n",
    "\n",
    "case_path_sql = (f'''\n",
    "    SELECT participant_id, filename, file_path\n",
    "    FROM genome_file_paths_and_types\n",
    "    WHERE participant_id IN {*all_als,}\n",
    "    AND file_sub_type = '{filetype}'\n",
    "''')\n",
    "\n",
    "case_path_query = labkey_to_df(case_path_sql, version, 100000)\n",
    "case_path_query['case'] = \"Yes\"\n",
    "\n",
    "control_path_sql = (f'''\n",
    "    SELECT participant_id, filename, file_path\n",
    "    FROM genome_file_paths_and_types\n",
    "    WHERE participant_id IN {*control_list,}\n",
    "    AND file_sub_type = '{filetype}'\n",
    "''')\n",
    "\n",
    "control_path_query = labkey_to_df(control_path_sql, version, 100000)\n",
    "control_path_query['case'] = \"No\"\n",
    "\n",
    "phenofile = pd.concat([case_path_query, control_path_query])\n",
    "phenofile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in PRS stats data <a name=\"prs-data\"></a>\n",
    "\n",
    "There is no way to bring s3 buckets into RE1. You can add the data to the docker image, [bring it in using Airlock](https://research-help.genomicsengland.co.uk/display/GERE/How+to+use+the+Airlock+tool+to+import+and+export+files), or clone it in from Git on the HPC. This will be located in your secure df_BMS folder and nobody else will have access.\n",
    "\n",
    "### Import Docker image using Singularity <a name=\"singularity\"></a>\n",
    "\n",
    "You can import your Docker image using Singularity, which is preinstalled on the HPC. There are instructions on using Singularity on the HPC [here](https://research-help.genomicsengland.co.uk/display/GERE/Using+containers+within+the+Research+Environment).\n",
    "\n",
    "Start by logging into the HPC with:\n",
    "\n",
    "`ssh <username>@corp.gel.ac@phpgridzlogn004.int.corp.gel.ac`\n",
    "\n",
    "You will need to cd into your working directory:\n",
    "\n",
    "`cd df_bms`\n",
    "\n",
    "From here you can load Singularity:\n",
    "\n",
    "`module load tools/singularity/3.8.3`\n",
    "\n",
    "You can pull your docker image with singularity:\n",
    "\n",
    "`singularity pull <docker image>`\n",
    "\n",
    "This will create a singularity image in your working directory. You can now run the container with:\n",
    "\n",
    "`singularity run <singularity container>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Germline variants in oncology cohorts <a name=\"germline-survival\"></a>\n",
    "\n",
    "Task:\n",
    "* Estimate _CTLA4_ and _FCGR_ specific germline variant effect on survival in cancer\n",
    "* To our knowledge this would allow us to understand the mechanism to extract germline genotypes (that are not in the small subset of annotated risk factors) and to correlate this with possible response ICI treatment\n",
    "\n",
    "### Build cohorts of participants with particular germline variants <a name=\"germline-cohort\"></a>\n",
    "\n",
    "We can use the [gene variant workflow](https://research-help.genomicsengland.co.uk/display/GERE/Gene-Variant+Workflow) to find all participants with variants in a gene.\n",
    "\n",
    "1. Load copy the script as described in the documentation.\n",
    "2. Edit the gene list to the two genes, and the submit_workflow.sh and variant_workflow_inputs.json files to have the correct project codes.\n",
    "3. Submit the job as described in the documentation.\n",
    "\n",
    "The results will appear in the folder `final_output/data`, in files named like `chr<chr>_<gene_name>_<Ensembl_ID>_<genome_assembly>_annotated_variants.tsv`.\n",
    "\n",
    "There will be one table for GRCh37 and one for GRCh38. All of the cancer participants have their genomes aligned to GRCh38, so you can ignore the GRCh37 files.\n",
    "\n",
    "This will not filter by rare disease vs cancer so we will need to use LabKey to pull out cancer participants. We'll start by making importing the table as a dataframe and pulling out the relevant data: I've gone for the location, alleles, consequences and samples. \n",
    "\n",
    "I'm also filtering to only get the consequence on the canonical transcript. I've used a further filter to only get variants with certain consequences. Edit the list `consequences` to find the consequences you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_gene_variant_results = pd.read_csv(\n",
    "    'genevariant/final_output/data/'\n",
    "    'chr2_CTLA4_ENSG00000163599_GRCh38_annotated_variants.tsv' , \n",
    "    sep='\\t')\n",
    "consequences = [\n",
    "    \"missense_variant\", \n",
    "    \"transcript_ablation\", \n",
    "    \"splice_donor_variant\", \n",
    "    \"splice_acceptor_variant\", \n",
    "    \"frameshift_variant\", \n",
    "    \"stop_lost\", \n",
    "    \"start_lost\", \n",
    "    \"stop_gained\", \n",
    "    \"inframe_deletion\", \n",
    "    \"inframe_insertion\"\n",
    "    ]\n",
    "full_gene_variant_results['in_list'] = (\n",
    "    full_gene_variant_results\n",
    "    .Consequence_annotation\n",
    "    .apply(\n",
    "        lambda x: any(i in x for i in consequences)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "full_gene_variant_results\n",
    "filtered_gv = full_gene_variant_results[\n",
    "    (full_gene_variant_results['CANONICAL_annotation']=='YES') \n",
    "    & (full_gene_variant_results['in_list']==True)\n",
    "    ]\n",
    "short_gv = filtered_gv[[\n",
    "    'ID_variant', \n",
    "    'REF_variant', \n",
    "    'ALT_variant', \n",
    "    'Consequence_annotation', \n",
    "    'Het_Samples', \n",
    "    'Hom_Samples']]\n",
    "short_gv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to wrangle so that there's one platekey per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_gv = pd.melt(\n",
    "    short_gv, \n",
    "    id_vars = [\n",
    "        'ID_variant',\n",
    "        'REF_variant',\n",
    "        'ALT_variant',\n",
    "        'Consequence_annotation'\n",
    "        ],\n",
    "    value_vars = [\n",
    "        'Het_Samples', \n",
    "        'Hom_Samples'\n",
    "        ],\n",
    "    var_name = 'zygosity',\n",
    "    value_name = 'platekey'\n",
    "    )\n",
    "\n",
    "short_gv = short_gv.dropna()\n",
    "\n",
    "short_gv['platekey'] = short_gv['platekey'].str.split(\",\")\n",
    "short_gv = short_gv.explode('platekey')\n",
    "short_gv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look these Platekeys up in the Labkey cancer_analysis table, to filter to only cancer participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platekeys = set(list(short_gv['platekey']))\n",
    "\n",
    "cancer_sql = (\n",
    "    f'''\n",
    "    SELECT participant_id, germline_sample_platekey\n",
    "    FROM cancer_analysis\n",
    "    WHERE germline_sample_platekey IN  {*platekeys,}\n",
    "    '''\n",
    ")\n",
    "cancer_query = labkey_to_df(cancer_sql, version, 100000)\n",
    "cancer_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from LabKey for survival analysis <a name=\"survival-data\"></a>\n",
    "\n",
    "For survial analysis, you will need to find the birth, death and diagnosis dates of the participants. You may also wish to fetch covariate information such as sex, ethnicity and principal components. The following code fetches the dates for the list specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctla4_part = set(list(cancer_query['participant_id']))\n",
    "\n",
    "# import the python survival analysis functions:\n",
    "import sys\n",
    "sys.path.append('/pgen_int_work/BRS/christian/python_survival/')\n",
    "import survival as su\n",
    "\n",
    "# comparing on presence/absence of germline mutation in the entire 100K\n",
    "# cancer cohort.\n",
    "# you can ofcourse limit your cohort to, for example, only those who have\n",
    "# received immune checkpoint blockade here.\n",
    "sqlstr = ('''\n",
    "    SELECT\n",
    "        participant_id, disease_type\n",
    "    FROM\n",
    "        lists.cancer_analysis\n",
    "    WHERE\n",
    "        tumour_type = 'PRIMARY' ''')\n",
    "ca = labkey_to_df(\n",
    "    sqlstr,\n",
    "    version, \n",
    "    100000)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate the survival class, through which we can extract:\n",
    "- mortality: quer_ons()\n",
    "- date of last follow up: quer_hes()\n",
    "- date of diagnosis: quer_dod()\n",
    "- match diagnosis date with cohort based on disease type: merge_dod()\n",
    "- impute diagnosis date based on average per disease types for those with no diagnosis date: dod_impute()\n",
    "- generate survival data from the above: surv_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = su.Survdat(\n",
    "    ca, \n",
    "    ca['participant_id'],\n",
    "    '/main-programme/main-programme_v16_2022-10-13',\n",
    "    impute=False)\n",
    "c.quer_ons()  # get mortality data : c.ons\n",
    "c.quer_hes()  # query HES data for date of last follow up : c.hes\n",
    "c.quer_dod()  # get date of diagnosis :c.dod\n",
    "c.merge_dod()  # match date of diagnosis with cohort: c.pid_diag, c.no_diag\n",
    "c.dod_impute()  # date of diagnosis from average per disease type c.full_diag\n",
    "# impute is only appended to c.pid_diag if impute=True.\n",
    "c.surv_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now include the germline data, this can be multiple genes, each as its own True/False column.\n",
    "With these columns a Group column can be made with the su.assign_groups()\n",
    "\n",
    "This function assigns groups as follows:\n",
    "'and' = Group 1 if all germline data is mutated (True)\n",
    "'or' = Group 1 if any germline data is muated (True)\n",
    "'full' = each combination of True/False for multiple genes is assigned its own group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.surv_dat['ctla4'] = c.surv_dat['participant_id'].isin(ctla4_part)\n",
    "\n",
    "grouped_surv_dat = su.assign_groups(\n",
    "    dataframe=c.surv_dat,\n",
    "    vars=['ctla4'],\n",
    "    type='or'\n",
    "    )\n",
    "grouped_surv_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally plot the Kaplan-Meier curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su.kmsurvival(\n",
    "    data=grouped_surv_dat,\n",
    "    strata=pd.unique(grouped_surv_dat['group']),\n",
    "    output='./',\n",
    "    plt_title='survival stratified by germline CTLA4 mutations',\n",
    "    plot=True,\n",
    "    table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariate information is available in the `aggregate_gvcf_sample_stats` table. We're going to fetch sex, ethnicity and principal components for our list of participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs = [\"pc\" + str(i+1) for i in range(20)]\n",
    "\n",
    "cov_sql = (f''' SELECT participant_id,\n",
    "    karyotype, {\", \".join(str(x) for x in PCs)},\n",
    "    pred_african_ancestries as AFR, \n",
    "    pred_south_asian_ancestries as SAS, \n",
    "    pred_east_asian_ancestries as EAS, \n",
    "    pred_european_ancestries as EUR, \n",
    "    pred_american_ancestries as AMR\n",
    "    FROM aggregate_gvcf_sample_stats\n",
    "    WHERE participant_id IN {*ctla4_part,}\n",
    "    ''')\n",
    "\n",
    "cov_query = labkey_to_df(cov_sql, version, 10000)\n",
    "cov_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using the genomically determined ethnicity to filter your cohort. As a general rule:\n",
    "* If score â‰¥ 0.8, participant is this population\n",
    "* If all scores <0.8, participant is admixed\n",
    "\n",
    "The following code calculates the genomic ethnicity, following these rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_eth = [\"EUR\", \"SAS\", \"EAS\", \"AFR\", \"AMR\"]\n",
    "\n",
    "cov_query['max_eth'] = cov_query[gen_eth].max(axis=\"columns\")\n",
    "cov_query['max_eth_index'] = cov_query[gen_eth].idxmax(axis=\"columns\")\n",
    "\n",
    "cov_query['gen_eth'] = cov_query.apply(\n",
    "    lambda x: \n",
    "        x['max_eth_index'] if x['max_eth'] >= 0.8 else 'Admixed',\n",
    "     axis=1)\n",
    "cov_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use these data for survival analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Somatic variants association with survival <a name=\"somatic-survival\"></a>\n",
    "\n",
    "Task\n",
    "* Effect of somatic STK11 mutations on survival in NSCLC patients\n",
    "\n",
    "This can be done using the [R survival analysis](https://research-help.genomicsengland.co.uk/display/GERE/Survival+-+cancer) script, which carries out survival analysis comparing participants with somatic variants in a particular gene to those without.\n",
    "\n",
    "1. Load copy the script as described in the documentation.\n",
    "2. Edit line 350 to your gene of interest\n",
    "3. Run the script with R\n",
    "\n",
    "The same can be achieved with the Python survival analysis script, set up the submit_surv.sh: \n",
    "\n",
    "python3 /pgen_int_work/BRS/christian/python_survival/survival.py \\\n",
    "    --genes STK11 \\\n",
    "    --disease LUNG \\\n",
    "    -s or \\\n",
    "    --imputate \\\n",
    "    -o ./surv_out/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
